total_events_now = treatment_events + control_events
a_posterior = a0 + treatment_events
b_posterior = b0 + control_events
upper = qbeta(1-alpha_interim[qqq], a_posterior, b_posterior)
if(qqq < k_use){
non_inferiority = (upper <= theta_NI)
futility = pbbinom(treatment_max - treatment_events, total_events - total_events_now,a_posterior,b_posterior)  < futility_prob_threshold
} else{
non_inferiority = (upper <= theta_NI)
futility = (upper > theta_NI)
}
return(c(days = interim_days[qqq],
events = total_events_now,
enrolled = sum(tmp$total_enrolled),
non_inferiority = non_inferiority,
futility = futility))
}) %>%   `colnames<-`(paste("Interim",1:k_use))
##### Final analysis --- no interims
tmp = dat_groups %>% filter(day == interim_days[k_use])
treatment_events = tmp$total_infected[tmp$group == "treatment"]
control_events = tmp$total_infected[tmp$group == "control"]
total_events_now = treatment_events + control_events
a_posterior = a0 + treatment_events
b_posterior = b0 + control_events
upper = qbeta(1-alpha, a_posterior, b_posterior)
non_inferiority = (upper <= theta_NI)
futility = (upper > theta_NI)
final_only = c(days = interim_days[k_use],
events = total_events_now,
enrolled = sum(tmp$total_enrolled),
non_inferiority = non_inferiority,
futility = futility)
return(cbind(interim_results,`Final Only` = final_only))
},mc.cores =  10)
out_res = do.call(rbind, lapply(1:n_trial,function(qqq){
interim_stop = which(apply(sims[[qqq]][c("non_inferiority","futility"),],2,sum)  > 0)[1]
result = which(sims[[qqq]][c("non_inferiority","futility"),interim_stop] == 1)
return(data.frame(
interim = interim_stop,
result = ifelse(names(result) == "non_inferiority",1,0),
days = sims[[qqq]]["days",interim_stop],
events = sims[[qqq]]["events",interim_stop],
enrolled = sims[[qqq]]["enrolled",interim_stop],
enrolled_by_end = sims[[qqq]]["enrolled",k_use],
regret_stopping = ifelse(names(result) == "futility" &  sum(sims[[qqq]][c("non_inferiority"),]) > 0,TRUE,FALSE),
no_interim_result = ifelse(names(which(sims[[qqq]][c("non_inferiority","futility"),"Final Only"] == 1)) == "non_inferiority",1,0)
))
}))
sim_adaptive_trial = function(
ratio = 1 / 1 ,
alpha = .025,
power = .8,
futility_prob_threshold = 0.1,
assumed_Pfizer_ve = 0.8,
required_effect_preserved = 0.8,
total_events = 300,
HR_sim = 1,
attack_rate_pfizer = 0.03,
follow_up_period = 365,
recruitment_period = 365*2,
interim_timing = c(1/2,3/4,1),
target_subjects = 13000,
n_trial = 1e3){
hr_95 = 1 - assumed_Pfizer_ve
hr_NI = (hr_95)^(-(1-required_effect_preserved))
ve_NI = 1-hr_NI    #' relative vaccine efficacy non-inferiority margin (HR derivation below)
theta_NI = ve_to_theta(ve_NI, ratio)
### Priors for beta of Pr(Assigned to Treatment | Patient Has Event)
a0 = 1/2
b0 = 1/2
### Figure out cutoffs for posterior predicitve probabilities
treatment_hypotheticals =  0:total_events
#' Posterior
a_posterior = a0 + treatment_hypotheticals
b_posterior = b0 + total_events -treatment_hypotheticals
upper_hypothetical = qbeta(1-alpha, a_posterior, b_posterior)
treatment_max = max(treatment_hypotheticals[upper_hypothetical <= theta_NI])  #' Largest number of treatment events that still shows non-inferiority
##### ##### ##### ##### ##### ##### ##### ##### ##### ##### ##### #####
##### Attack rate. -- This could build in VE or waning VE, or seasonality
##### ##### ##### ##### ##### ##### ##### ##### ##### ##### ##### #####
attack_rate_exp = attack_rate_pfizer * HR_sim    ### per follow-up period
attack_con_tre = c(control = attack_rate_pfizer, treatment = attack_rate_exp)
##### ##### ##### ##### ##### ##### ##### ##### #####
##### ##### ##### ##### ##### ##### ##### ##### #####
##### ##### ##### ##### ##### ##### ##### ##### #####
full_period = follow_up_period + recruitment_period
days_numeric = 1:full_period
all_dates = mdy("12/31/2023") + days_numeric
recruit_dates = mdy("12/31/2023") + 1:recruitment_period
accrual_rate_overall = target_subjects/recruitment_period       # attack rate per day
accrual_function = function(x,season_amp = 0.5,phase_shift = -1*pi/6,ramp_up = 100){
normalize = integrate(function(xx){
exp(season_amp*cos(xx * 2 * pi / 365 - phase_shift))* ifelse(xx/ramp_up>1,1,xx/ramp_up)
},0,recruitment_period)$value/recruitment_period
accrual_rate_overall*exp(season_amp*cos(x * 2 * pi / 365- phase_shift))* ifelse(x/ramp_up>1,1,x/ramp_up)/normalize
}
accrual_function = function(x,ramp_up = 100){
normalize = integrate(function(xx){
ifelse(xx/ramp_up>1,1,xx/ramp_up)
},0,recruitment_period)$value/recruitment_period
accrual_rate_overall * ifelse(x/ramp_up>1,1,x/ramp_up)/normalize
}
accrual_function = function(x,ramp_up = 100){
rep(accrual_rate_overall,length(x))
}
# integrate(accrual_function,0,recruitment_period)
accrual_rate = accrual_function(1:recruitment_period)
##### ##### ##### ##### ##### ##### ##### ##### #####
##### Compliance with second dose - not used right now
##### ##### ##### ##### ##### ##### ##### ##### #####
num_shots = 2
shot_spacing = 28              # days
prob_get_second_dose = 1       # probability of getting second dose
##### ##### ##### ##### ##### ##### ##### ##### #####
##### Number of trials simulated
##### ##### ##### ##### ##### ##### ##### ##### #####
check_ins = total_events * interim_timing
### final interim is the final analysis
##### Show them how the spending works.
##### Show them power with/without interims
##### Show them change in sample size
k_use = length(check_ins)
bound = gsDesign(
k = k_use,              ### number of check_ins
test.type = 1,      ### one-sided test
alpha = alpha,      ### type-I error
beta = 1-power,        ### 1- power
n.fix = total_events,        ### fixed sample size
timing = interim_timing,     ### Information fraction
sfu = 'OF'          ### spending function
)
alpha_interim = 1 - pnorm(bound$upper$bound)
##### ##### ##### ##### ##### ##### ##### ##### #####
##### Patient start times
##### ##### ##### ##### ##### ##### ##### ##### #####
sims = mclapply(1:n_trial,function(index){
n_subjects = rpois(1,integrate(accrual_function,0,recruitment_period)$value)
n_treatment = round(n_subjects * ratio/(1+ratio))
data_sim = tibble(
start_day = sample(1:recruitment_period,n_subjects,replace = TRUE,prob = accrual_rate),
group = sample(c(rep("treatment", n_treatment),rep("control",n_subjects - n_treatment))),
infected =  rbinom(n_subjects,1,prob = attack_con_tre[group])) %>%
mutate(infect_time = ifelse(infected == 1, round(start_day + follow_up_period * runif(n_subjects)),NA)) %>%
arrange(group,start_day)
tmp1 = data_sim %>% #filter(start_day == ii) %>%
group_by(start_day,group) %>%
summarize(enrolled = n(), .groups = 'drop') %>%
complete(start_day=1:full_period,group, fill = list(enrolled = 0)) %>%
group_by(group) %>%
mutate(total_enrolled = cumsum(enrolled))
tmp2 = data_sim %>% #filter(start_day == ii) %>%
group_by(infect_time,group) %>%
summarize(infected = n(), .groups = 'drop') %>%
complete(infect_time = 1:full_period,group, fill = list(infected = 0)) %>%
group_by(group) %>%
mutate(total_infected = cumsum(infected))
dat_groups = expand_grid(day = 1:full_period,group = c("control","treatment")) %>%
left_join(tmp1,by = c("day" = "start_day","group"="group")) %>%
left_join(tmp2,by = c("day" = "infect_time","group"="group"))
dat_all =  dat_groups %>%
group_by(day) %>%
summarise(enrolled = sum(enrolled),
total_enrolled = sum(total_enrolled),
infected = sum(infected),
total_infected = sum(total_infected)) %>%
mutate(group = "all") %>%
arrange(day,group)
interim_days = sapply(check_ins,function(qqq){
dat_all$day[which(dat_all$total_infected >= qqq)[1]]
})
interim_days = ifelse(is.na(interim_days),full_period,interim_days)
interim_results = sapply(1:k_use,function(qqq){
tmp = dat_groups %>% filter(day == interim_days[qqq])
treatment_events = tmp$total_infected[tmp$group == "treatment"]
control_events = tmp$total_infected[tmp$group == "control"]
total_events_now = treatment_events + control_events
a_posterior = a0 + treatment_events
b_posterior = b0 + control_events
upper = qbeta(1-alpha_interim[qqq], a_posterior, b_posterior)
if(qqq < k_use){
non_inferiority = (upper <= theta_NI)
futility = pbbinom(treatment_max - treatment_events, total_events - total_events_now,a_posterior,b_posterior)  < futility_prob_threshold
} else{
non_inferiority = (upper <= theta_NI)
futility = (upper > theta_NI)
}
return(c(days = interim_days[qqq],
events = total_events_now,
enrolled = sum(tmp$total_enrolled),
non_inferiority = non_inferiority,
futility = futility))
}) %>%   `colnames<-`(paste("Interim",1:k_use))
##### Final analysis --- no interims
tmp = dat_groups %>% filter(day == interim_days[k_use])
treatment_events = tmp$total_infected[tmp$group == "treatment"]
control_events = tmp$total_infected[tmp$group == "control"]
total_events_now = treatment_events + control_events
a_posterior = a0 + treatment_events
b_posterior = b0 + control_events
upper = qbeta(1-alpha, a_posterior, b_posterior)
non_inferiority = (upper <= theta_NI)
futility = (upper > theta_NI)
final_only = c(days = interim_days[k_use],
events = total_events_now,
enrolled = sum(tmp$total_enrolled),
non_inferiority = non_inferiority,
futility = futility)
return(cbind(interim_results,`Final Only` = final_only))
},mc.cores =  10)
#### Calculated number of early futilities that would have been successful
out_res = do.call(rbind, lapply(1:n_trial,function(qqq){
interim_stop = which(apply(sims[[qqq]][c("non_inferiority","futility"),],2,sum)  > 0)[1]
result = which(sims[[qqq]][c("non_inferiority","futility"),interim_stop] == 1)
return(data.frame(
interim = interim_stop,
result = ifelse(names(result) == "non_inferiority",1,0),
days = sims[[qqq]]["days",interim_stop],
events = sims[[qqq]]["events",interim_stop],
enrolled = sims[[qqq]]["enrolled",interim_stop],
enrolled_by_end = sims[[qqq]]["enrolled",k_use],
regret_stopping = ifelse(names(result) == "futility" &  sum(sims[[qqq]][c("non_inferiority"),]) > 0,TRUE,FALSE),
no_interim_result = ifelse(names(which(sims[[qqq]][c("non_inferiority","futility"),"Final Only"] == 1)) == "non_inferiority",1,0)
))
}))
return(out_res)
}
tmp = sim_adaptive_trial()
tmp
.07 * 200
setwd("/Volumes/philip/BNP_hypertorus/code")
choose(6,3)/choose(12,3)
choose(6,3)
6*5*4
.36 * .68*.4
library(splines)
library(fields)
library(splines2)
library(nimble)
library(vegan)
library(gdm)
library(sf)
library(tidyverse)
rm(list = ls())
#----------------------------------------------------------------
# load in and parse data
#----------------------------------------------------------------
data(southwest)
# Parse data into location, environmental variables, and cover/presence data
tmp = sf::sf_project(from = "EPSG:4326", to = "EPSG:32651",
cbind( southwest$Long,southwest$Lat))
## Eastings and Northings
southwest$x = tmp[,1]/1000.
southwest$y = tmp[,2]/1000
southwest$present = 1
sppData = southwest[c(1,2,15,16)]
envTab = southwest[c(2:12)]
# sitePairTab = formatsitepair(sppData,2,XColumn="x",YColumn="y",sppColumn="species",
#                              siteColumn="site",predData=envTab)
south_block = southwest%>%
group_by(site,x,y,phTotal,bio5,bio19,species) %>%
dplyr::summarize(isPresent = mean(present)) %>%
spread(species, isPresent,fill = 0) #%>%
location_mat = south_block[,2:3]
envr_use = south_block[,4:6]
species_mat = south_block[,-(1:6)]
# save number of sites
ns = nrow(location_mat)
#----------------------------------------------------------------
# Calculate Bray-Curtis dissimilarity -- see proportion of 0's and 1's
#----------------------------------------------------------------
dist_use = as.matrix(vegdist(species_mat,"bray"))
Z = dist_use[upper.tri(dist_use)]
# index for those which are exactly one
Z_is_one = which(Z == 1)
Z_is_not_one = which(Z != 1)
# get counts
n1 = length(Z_is_one)
N = length(Z)
mean(Z == 0)
mean(Z == 1)
#----------------------------------------------------------------
# Define covariates that will be warped
#----------------------------------------------------------------
# Calculate geographical distance in km
dist_mat = as.matrix(rdist(cbind(location_mat$x,location_mat$y)))
vec_distance = dist_mat[upper.tri(dist_mat)]
# Define X to be environmental variables or a subset of them.
# How many knots do you want? What is the degree of the spline?
# Remember that in the specification, of the iSpline that the degree is
# one higher that what you say. Integration of m-spline adds one degree.
X = envr_use
deg = 3
knots = 1
df_use = deg + knots
formula_use = as.formula(paste("~ 0 +",paste(
paste("iSpline(`",colnames(X),"`,degree=",deg - 1 ,",df = ",df_use,
" ,intercept = TRUE)",sep = ""),collapse = "+")))
# combine distance and environmental I-spline bases
I_spline_bases = model.matrix(formula_use,data = X)
X_GDM = cbind(sapply(1:ncol(I_spline_bases),function(i){
dist_temp = rdist(I_spline_bases[,i])
vec_dist = dist_temp[upper.tri(dist_temp)]
vec_dist
}),
iSpline(vec_distance,degree = deg -1,
df = df_use,intercept = TRUE)
)
p = ncol(X_GDM)
colnames(X_GDM) = c(
paste(rep(colnames(X),each = df_use ),"I",rep(1:df_use,times = ncol(X)),sep = ""),
paste("dist","I",1:df_use,sep = "")
)
### Associate each dissimilarity with two sites (row and col index)
tmp = matrix(rep(1:nrow(dist_use),each = nrow(dist_use)),nrow = nrow(dist_use))
col_ind = tmp[upper.tri(tmp)]
tmp = matrix(rep(1:nrow(dist_use),times = nrow(dist_use)),nrow = nrow(dist_use))
row_ind = tmp[upper.tri(tmp)]
#------------------------------------------------------------------------
# Get Initial values for modeling fitting
#------------------------------------------------------------------------
lm_mod= lm(log(Z) ~ X_GDM)
lm_out = optim(c(.3, ifelse(coef(lm_mod)[-1]> 0,log(coef(lm_mod)[-1]), -10),rnorm(ns)) ,function(par){
sum((log(Z) - par[1] - X_GDM %*% exp(par[2:(p + 1)]) -
(par[p+1+row_ind] - par[p+1 + col_ind])^2 )^2)
},method = "BFGS")
#------------------------------------------------------------------------
# Fix spatial range parameter (rho = 1 / phi)
#------------------------------------------------------------------------
rho_fix = max(dist_mat)/10
R_spat = exp(-dist_mat/rho_fix)
chol_R = t(chol(R_spat))
R_inv = solve(R_spat)
#------------------------------------------------------------------------
# Define design matrix for polynomial log-variance
#------------------------------------------------------------------------
X_sigma = cbind(1,poly(vec_distance,degree = 3))
p_sigma = ncol(X_sigma)
#------------------------------------------------------------------------
# Source nimble models -- Models 1-9 match those in paper
#------------------------------------------------------------------------
source("nimble_models.R")
setwd("~/Documents/GitHub/gsGDM-code/nimble_code")
library(splines)
library(fields)
library(splines2)
library(nimble)
library(vegan)
library(gdm)
library(sf)
library(tidyverse)
rm(list = ls())
#----------------------------------------------------------------
# load in and parse data
#----------------------------------------------------------------
data(southwest)
# Parse data into location, environmental variables, and cover/presence data
tmp = sf::sf_project(from = "EPSG:4326", to = "EPSG:32651",
cbind( southwest$Long,southwest$Lat))
## Eastings and Northings
southwest$x = tmp[,1]/1000.
southwest$y = tmp[,2]/1000
southwest$present = 1
sppData = southwest[c(1,2,15,16)]
envTab = southwest[c(2:12)]
# sitePairTab = formatsitepair(sppData,2,XColumn="x",YColumn="y",sppColumn="species",
#                              siteColumn="site",predData=envTab)
south_block = southwest%>%
group_by(site,x,y,phTotal,bio5,bio19,species) %>%
dplyr::summarize(isPresent = mean(present)) %>%
spread(species, isPresent,fill = 0) #%>%
location_mat = south_block[,2:3]
envr_use = south_block[,4:6]
species_mat = south_block[,-(1:6)]
# save number of sites
ns = nrow(location_mat)
#----------------------------------------------------------------
# Calculate Bray-Curtis dissimilarity -- see proportion of 0's and 1's
#----------------------------------------------------------------
dist_use = as.matrix(vegdist(species_mat,"bray"))
Z = dist_use[upper.tri(dist_use)]
# index for those which are exactly one
Z_is_one = which(Z == 1)
Z_is_not_one = which(Z != 1)
# get counts
n1 = length(Z_is_one)
N = length(Z)
mean(Z == 0)
mean(Z == 1)
#----------------------------------------------------------------
# Define covariates that will be warped
#----------------------------------------------------------------
# Calculate geographical distance in km
dist_mat = as.matrix(rdist(cbind(location_mat$x,location_mat$y)))
vec_distance = dist_mat[upper.tri(dist_mat)]
# Define X to be environmental variables or a subset of them.
# How many knots do you want? What is the degree of the spline?
# Remember that in the specification, of the iSpline that the degree is
# one higher that what you say. Integration of m-spline adds one degree.
X = envr_use
deg = 3
knots = 1
df_use = deg + knots
formula_use = as.formula(paste("~ 0 +",paste(
paste("iSpline(`",colnames(X),"`,degree=",deg - 1 ,",df = ",df_use,
" ,intercept = TRUE)",sep = ""),collapse = "+")))
# combine distance and environmental I-spline bases
I_spline_bases = model.matrix(formula_use,data = X)
X_GDM = cbind(sapply(1:ncol(I_spline_bases),function(i){
dist_temp = rdist(I_spline_bases[,i])
vec_dist = dist_temp[upper.tri(dist_temp)]
vec_dist
}),
iSpline(vec_distance,degree = deg -1,
df = df_use,intercept = TRUE)
)
p = ncol(X_GDM)
colnames(X_GDM) = c(
paste(rep(colnames(X),each = df_use ),"I",rep(1:df_use,times = ncol(X)),sep = ""),
paste("dist","I",1:df_use,sep = "")
)
### Associate each dissimilarity with two sites (row and col index)
tmp = matrix(rep(1:nrow(dist_use),each = nrow(dist_use)),nrow = nrow(dist_use))
col_ind = tmp[upper.tri(tmp)]
tmp = matrix(rep(1:nrow(dist_use),times = nrow(dist_use)),nrow = nrow(dist_use))
row_ind = tmp[upper.tri(tmp)]
#------------------------------------------------------------------------
# Get Initial values for modeling fitting
#------------------------------------------------------------------------
lm_mod= lm(log(Z) ~ X_GDM)
lm_out = optim(c(.3, ifelse(coef(lm_mod)[-1]> 0,log(coef(lm_mod)[-1]), -10),rnorm(ns)) ,function(par){
sum((log(Z) - par[1] - X_GDM %*% exp(par[2:(p + 1)]) -
(par[p+1+row_ind] - par[p+1 + col_ind])^2 )^2)
},method = "BFGS")
#------------------------------------------------------------------------
# Fix spatial range parameter (rho = 1 / phi)
#------------------------------------------------------------------------
rho_fix = max(dist_mat)/10
R_spat = exp(-dist_mat/rho_fix)
chol_R = t(chol(R_spat))
R_inv = solve(R_spat)
#------------------------------------------------------------------------
# Define design matrix for polynomial log-variance
#------------------------------------------------------------------------
X_sigma = cbind(1,poly(vec_distance,degree = 3))
p_sigma = ncol(X_sigma)
#------------------------------------------------------------------------
# Source nimble models -- Models 1-9 match those in paper
#------------------------------------------------------------------------
source("nimble_models.R")
# create constants for nimble model
constants <- list(n = N, p = p, x = X_GDM,n_loc = ns,
p_sigma = p_sigma,X_sigma = X_sigma,R_inv = R_inv,
zeros = rep(0, ns),row_ind = row_ind, col_ind = col_ind)
# create data for nimble model
data <- list(log_V = ifelse(Z == 1,NA, log(Z)),
censored = 1*(Z == 1),
c = rep(0,constants$n))
# create initial values for nimble model -- this will change depending on model
inits <- list(beta_0 = lm_out$par[1],
log_beta = lm_out$par[2:(p+1)],
sig2_psi = 1,
beta_sigma = c(-5,-20,12,2),
psi = lm_out$par[-(1:(p+1))])
#### "nimble_code8" is model 8 in paper. Change to what you want in nimble_models.R.
model <- nimbleModel(nimble_code8, constants = constants, data = data, inits = inits)
mcmcConf <- configureMCMC(model)
# Block sampler for beta_0, log(\beta_{jk}), and \beta_{\sigma}
# MCMC may work better including psi in this blocking
# Some models (1,4,7) won't have beta_sigma
mcmcConf$removeSamplers(c("beta_0",'log_beta',"psi","sig2_psi",'beta_sigma'))
# mcmcConf$addSampler(target = c("beta_0",'log_beta',"beta_sigma"), type = 'RW_block')
mcmcConf$addSampler(target = c("beta_0",'log_beta',"psi","sig2_psi",'beta_sigma'),
type = 'AF_slice')
# May need to change depending on model
# For example, models 1, 4, and 7 will have "sigma2" instead of "beta_sigma"
# For example, models 1, 2, and 3 will not have "psi"
mcmcConf$addMonitors(c('beta_0','beta','beta_sigma','psi',"sig2_psi"))
# mcmcConf$enableWAIC = TRUE
codeMCMC <- buildMCMC(mcmcConf)
Cmodel = compileNimble(codeMCMC,model)
##### Run a super long MCMC
##### thin so that we get 10,000 posterior samples -- saves memory
n_tot = 2e3
n_burn = 1e3
n_post = n_tot - n_burn
# You may get some warnings because we didn't initialize log_V where Z = 1.
post_samples <- runMCMC(Cmodel$codeMCMC,niter = n_tot,nburnin = n_burn,
thin = 1)
##### A few trace plot
plot(post_samples[,"beta_0"],type= "l")
plot(post_samples[,"log_beta[9]"],type= "l")
plot(post_samples[,"beta[9]"],type= "l")
plot(post_samples[,"beta_sigma[2]"],type= "l")
plot(post_samples[,"psi[1]"],type= "l")
n_tot = 15e3
n_burn = 5e3
n_post = n_tot - n_burn
# You may get some warnings because we didn't initialize log_V where Z = 1.
post_samples <- runMCMC(Cmodel$codeMCMC,niter = n_tot,nburnin = n_burn,
thin = 1)
##### A few trace plot
plot(post_samples[,"beta_0"],type= "l")
plot(post_samples[,"log_beta[9]"],type= "l")
plot(post_samples[,"beta[9]"],type= "l")
plot(post_samples[,"beta_sigma[2]"],type= "l")
plot(post_samples[,"psi[1]"],type= "l")
plot(post_samples[,"sig2_psi"],type= "l")
exp(-10)
exp(-6)
exp(100)
exp(-900)
exp(-100)
